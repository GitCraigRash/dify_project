
import os
import re
import json
import yaml
import logging
import traceback
from typing import List, Dict, Any, Optional

class CodebaseAnalyzer:
    """Main class to analyze codebase and suggest file modifications"""
    
    def __init__(self, codebase_root: str, config_path: str = None):
        """
        Initialize the CodebaseAnalyzer with codebase root and optional configuration

        Args:
            codebase_root (str): Root directory of the project/codebase
            config_path (str, optional): Path to the configuration file. 
                                        Defaults to None.
        
        Attributes:
            codebase_root (str): Root directory of the project
            config_manager (ConfigurationManager): Manages configuration settings
            file_parser (FileParser): Handles file parsing and analysis
            dependency_analyzer (DependencyAnalyzer): Analyzes code dependencies
            logger (logging.Logger): Logging instance for tracking analysis process
        """
        # Validate and set codebase root
        if not os.path.exists(codebase_root):
            raise ValueError(f"Codebase root directory does not exist: {codebase_root}")
        
        self.codebase_root = os.path.abspath(codebase_root)
        
        # Initialize configuration manager
        self.config_manager = (
            ConfigurationManager(config_path) 
            if config_path and os.path.exists(config_path) 
            else ConfigurationManager()
        )
        
        # Initialize utility classes
        self.file_parser = FileParser()
        self.dependency_analyzer = DependencyAnalyzer()
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # Create console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Create formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        
        # Add handler to logger
        self.logger.addHandler(console_handler)
        
        # Log initialization
        self.logger.info(f"CodebaseAnalyzer initialized for: {self.codebase_root}")

    def analyze_feature_requirement(self, codebase_path: str, feature_description: str) -> Dict[str, Any]:
    """
    Comprehensively analyze feature requirements and identify necessary code modifications
    
    Args:
        codebase_path (str): Path to the project codebase
        feature_description (str): Detailed description of the feature to implement
    
    Returns:
        Dict containing analysis results with file modifications, dependencies, and recommendations
    """
    # Log the feature analysis start
    self.logger.info(f"Analyzing feature: {feature_description}")
    
    # Scan the entire codebase
    scanned_files = self.file_parser.scan_directory(codebase_path)
    
    # Identify relevant files based on feature description
    relevant_files = self._find_relevant_files(feature_description)
    
    # Analyze dependencies
    dependencies = self.dependency_analyzer.identify_dependencies(scanned_files)
    
    # Perform security analysis
    security_risks = SecurityAnalyzer().analyze_security_risks(relevant_files)
    
    # Analyze performance implications
    performance_impact = PerformanceAnalyzer().analyze_performance_impact(relevant_files)
    
    # Detailed file modification recommendations
    file_modifications = self._generate_modification_recommendations(
        relevant_files, 
        feature_description
    )
    
    # Compile comprehensive analysis results
    analysis_result = {
        "feature_description": feature_description,
        "relevant_files": relevant_files,
        "file_modifications": file_modifications,
        "dependencies": dependencies,
        "security_risks": security_risks,
        "performance_impact": performance_impact
    }
    
    # Log analysis completion
    self.logger.info(f"Feature analysis completed for: {feature_description}")
    
    return analysis_result



class ConfigurationManager:
def __init__(self, config_path=None):
    """
    Initialize ConfigurationManager with optional configuration file path.
    
    Args:
        config_path (str, optional): Path to the configuration file. 
                                     Defaults to None.
    
    Attributes:
        config (dict): Loaded configuration dictionary
        api_endpoints (dict): Dictionary of API endpoints
        default_config (dict): Default configuration settings
    """
    # Define default configuration
    self.default_config = {
        'api_endpoints': {
            'base_url': 'https://api.example.com/v1',
            'auth': '/auth/login',
            'users': '/users',
            'resources': '/resources'
        },
        'logging': {
            'level': 'INFO',
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        },
        'security': {
            'jwt_secret': None,
            'token_expiration': 3600  # 1 hour
        }
    }
    
    # Initialize configuration
    self.config = self.default_config.copy()
    
    # Load configuration if path is provided
    if config_path:
        self.config.update(self._load_config(config_path))
    
    # Extract API endpoints
    self.api_endpoints = self.config.get('api_endpoints', {})

def _load_config(self, config_path):
    """
    Load configuration from file based on file extension.
    
    Args:
        config_path (str): Path to the configuration file
    
    Returns:
        dict: Loaded configuration dictionary
    """
    import os
    import json
    import yaml
    
    # Get file extension
    _, ext = os.path.splitext(config_path)
    
    try:
        with open(config_path, 'r') as f:
            if ext.lower() in ['.yaml', '.yml']:
                return yaml.safe_load(f)
            elif ext.lower() == '.json':
                return json.load(f)
            else:
                raise ValueError(f"Unsupported configuration file type: {ext}")
    except FileNotFoundError:
        print(f"Configuration file not found: {config_path}")
        return {}
    except (json.JSONDecodeError, yaml.YAMLError) as e:
        print(f"Error parsing configuration file: {e}")
        return {}


class FileParser:
    """Parse and analyze project files"""
    
def scan_directory(self, path: str, 
                   include_extensions: List[str] = None, 
                   exclude_dirs: List[str] = None) -> List[Dict[str, Any]]:
    """
    Recursively scan directory and parse files with advanced filtering and parsing
    
    Args:
        path (str): Root directory path to scan
        include_extensions (List[str], optional): File extensions to include 
        exclude_dirs (List[str], optional): Directories to exclude from scanning
    
    Returns:
        List of dictionaries containing file metadata and content
    """
    # Default configurations if not provided
    include_extensions = include_extensions or [
        '.py', '.js', '.ts', '.html', '.css', '.json', 
        '.yaml', '.yml', '.md', '.txt', '.xml'
    ]
    exclude_dirs = exclude_dirs or [
        '.git', '.venv', 'node_modules', 'build', 'dist', 
        '__pycache__', '.pytest_cache', '.mypy_cache'
    ]
    
    files = []
    
    # Use os.walk for recursive directory traversal
    for root, dirs, filenames in os.walk(path):
        # Remove excluded directories
        dirs[:] = [d for d in dirs if d not in exclude_dirs]
        
        for filename in filenames:
            # Check file extension
            file_ext = os.path.splitext(filename)[1].lower()
            if file_ext not in include_extensions:
                continue
            
            full_path = os.path.join(root, filename)
            
            try:
                file_info = {
                    'path': full_path,
                    'filename': filename,
                    'directory': root,
                    'type': self._get_file_type(filename),
                    'extension': file_ext,
                    'size': os.path.getsize(full_path),
                    'content': self._read_file(full_path)
                }
                files.append(file_info)
            
            except (PermissionError, IOError) as e:
                # Log or handle file reading errors
                print(f"Error reading file {full_path}: {e}")
    
    return files

def _get_file_type(self, filename: str) -> str:
    """
    Determine file type based on extension and content
    
    Args:
        filename (str): Name of the file
    
    Returns:
        str: Categorized file type
    """
    ext = os.path.splitext(filename)[1].lower()
    
    file_type_map = {
        # Programming Languages
        '.py': 'Python',
        '.js': 'JavaScript',
        '.ts': 'TypeScript',
        '.java': 'Java',
        '.cpp': 'C++',
        '.c': 'C',
        '.rb': 'Ruby',
        
        # Web Technologies
        '.html': 'HTML',
        '.css': 'CSS',
        '.scss': 'SCSS',
        
        # Configuration
        '.json': 'JSON Configuration',
        '.yaml': 'YAML Configuration',
        '.yml': 'YAML Configuration',
        '.ini': 'INI Configuration',
        
        # Documentation
        '.md': 'Markdown',
        '.txt': 'Plain Text',
        '.rst': 'ReStructured Text',
        
        # Others
        '.xml': 'XML',
        '.log': 'Log File'
    }
    
    return file_type_map.get(ext, 'Unknown')

class DependencyAnalyzer:
    """Analyze code dependencies and relationships"""
    
def identify_dependencies(self, files: List[Dict[str, Any]]) -> Dict[str, List[str]]:
    """
    Identify inter-file dependencies by analyzing import statements and module references
    
    Args:
        files (List[Dict[str, Any]]): List of file information dictionaries
    
    Returns:
        Dict[str, List[str]]: A dictionary mapping files to their dependencies
    """
    dependencies = {}
    
    # Regular expressions for different language import patterns
    import_patterns = {
        'python': [
            r'^\s*import\s+(\w+)',
            r'^\s*from\s+(\w+)\s+import',
            r'^\s*from\s+\.(\w+)\s+import'
        ],
        'javascript': [
            r'^\s*import\s+.*\s+from\s+[\'"](.+)[\'"]',
            r'^\s*const\s+.*\s*=\s*require\([\'"](.+)[\'"]\)'
        ],
        'typescript': [
            r'^\s*import\s+.*\s+from\s+[\'"](.+)[\'"]',
            r'^\s*import\s+\{.*\}\s+from\s+[\'"](.+)[\'"]'
        ]
    }
    
    for file_info in files:
        file_path = file_info['path']
        file_type = file_info['type'].lower()
        file_content = file_info['content']
        
        # Select appropriate import patterns based on file type
        patterns = []
        if 'python' in file_type:
            patterns = import_patterns['python']
        elif 'javascript' in file_type:
            patterns = import_patterns['javascript']
        elif 'typescript' in file_type:
            patterns = import_patterns['typescript']
        
        # Extract dependencies using regex
        file_dependencies = set()
        for pattern in patterns:
            import_matches = re.findall(pattern, file_content, re.MULTILINE)
            file_dependencies.update(import_matches)
        
        # Resolve dependencies to full file paths
        resolved_dependencies = []
        for dep in file_dependencies:
            matching_files = [
                f['path'] for f in files 
                if dep in f['filename'] or dep in f['path']
            ]
            resolved_dependencies.extend(matching_files)
        
        # Store unique dependencies
        dependencies[file_path] = list(set(resolved_dependencies))
    
    # Advanced dependency analysis
    dependencies = self._enhance_dependency_analysis(dependencies, files)
    
    return dependencies


class SecurityAnalyzer:
    """Analyze security risks in code"""
    
    def analyze_security_risks(self, files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Comprehensively identify potential security vulnerabilities in files
        
        Args:
            files (List[Dict[str, Any]]): List of file information dictionaries
        
        Returns:
            List of dictionaries containing security risks
        """
        security_risks = []
        
        # Security risk patterns for different file types
        risk_patterns = {
            'python': {
                'hardcoded_credentials': r'(password|secret|api_key)\s*=\s*[\'"]([^\'"]+)[\'"]',
                'sql_injection': r'(\.execute\(.*%s|\.format\(.*%s)',
                'shell_injection': r'subprocess\.(?:call|run|Popen)\(.*\bshell\s*=\s*True',
                'deserialization_risk': r'pickle\.(?:load|loads)',
                'eval_usage': r'\beval\(',
                'input_validation': r'input\(|raw_input\('
            },
            'javascript': {
                'hardcoded_credentials': r'(password|secret|apiKey)\s*:\s*[\'"]([^\'"]+)[\'"]',
                'xss_risk': r'\.innerHTML\s*=|document\.write\(',
                'eval_usage': r'\beval\(',
                'command_injection': r'child_process\.exec\(',
                'insecure_random': r'Math\.random\(\)'
            },
            'general': {
                'sensitive_comments': r'(TODO:|FIXME:)\s*(password|secret|key|credentials)',
                'debug_leftovers': r'console\.log\(.*credentials',
                'potential_tokens': r'(token|jwt|secret).*=\s*[\'"][A-Za-z0-9+/=]{20,}[\'"]\s*'
            }
        }
        
        for file_info in files:
            file_path = file_info['path']
            file_type = file_info['type'].lower()
            file_content = file_info['content']
            
            # Determine risk patterns based on file type
            applicable_patterns = risk_patterns['general']
            if 'python' in file_type:
                applicable_patterns.update(risk_patterns['python'])
            elif 'javascript' in file_type or 'typescript' in file_type:
                applicable_patterns.update(risk_patterns['javascript'])
            
            # Scan for security risks
            file_risks = self._scan_file_for_risks(
                file_path, 
                file_content, 
                applicable_patterns
            )
            
            if file_risks:
                security_risks.extend(file_risks)
        
        # Advanced risk correlation and prioritization
        prioritized_risks = self._prioritize_risks(security_risks)
        
        return prioritized_risks



class PerformanceAnalyzer:
    """Analyze performance implications of code changes"""
    
    def analyze_performance_impact(self, files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Comprehensively estimate performance impact of proposed changes
        
        Args:
            files (List[Dict[str, Any]]): List of files to analyze
        
        Returns:
            Dict containing performance metrics and recommendations
        """
        performance_impact = {
            'overall_complexity': 0,
            'potential_bottlenecks': [],
            'memory_usage': {},
            'time_complexity': {},
            'recommendations': []
        }
        
        for file_info in files:
            file_path = file_info['path']
            file_content = file_info['content']
            file_type = file_info['type'].lower()
            
            # Analyze based on file type
            if 'python' in file_type:
                file_performance = self._analyze_python_performance(file_path, file_content)
                performance_impact['time_complexity'][file_path] = file_performance['time_complexity']
                performance_impact['memory_usage'][file_path] = file_performance['memory_usage']
                performance_impact['potential_bottlenecks'].extend(file_performance['bottlenecks'])
            
            # Add complexity score
            performance_impact['overall_complexity'] += self._calculate_code_complexity(file_content)
        
        # Generate performance recommendations
        performance_impact['recommendations'] = self._generate_performance_recommendations(
            performance_impact
        )
        
        return performance_impact

class FeatureAnalyzer:
    """Main interface for feature implementation analysis"""
    
    def __init__(self, codebase_root: str, config_path: str = None, logging_level: str = 'INFO'):
    """
    Initialize FeatureAnalyzer with configuration and setup

    Args:
        codebase_root (str): Root directory of the project/codebase
        config_path (str, optional): Path to configuration file. Defaults to None.
        logging_level (str, optional): Logging level for analysis. Defaults to 'INFO'.

    Attributes:
        codebase_root (str): Root directory of the project
        config_path (str): Path to configuration file
        codebase_analyzer (CodebaseAnalyzer): Instance for comprehensive codebase analysis
        result_formatter (ResultFormatter): Formats analysis results
        logger (logging.Logger): Logger for tracking analysis process
    """
    # Validate codebase root
    if not os.path.exists(codebase_root):
        raise ValueError(f"Codebase root directory does not exist: {codebase_root}")
    
    # Set attributes
    self.codebase_root = os.path.abspath(codebase_root)
    self.config_path = config_path

    # Setup logging
    self.logger = logging.getLogger(__name__)
    logging_levels = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL
    }
    self.logger.setLevel(logging_levels.get(logging_level.upper(), logging.INFO))

    # Configure console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging_levels.get(logging_level.upper(), logging.INFO))
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    self.logger.addHandler(console_handler)

    # Initialize core analysis components
    try:
        self.codebase_analyzer = CodebaseAnalyzer(
            codebase_root=self.codebase_root, 
            config_path=self.config_path
        )
        self.result_formatter = ResultFormatter()
        
        # Log successful initialization
        self.logger.info(f"FeatureAnalyzer initialized for: {self.codebase_root}")
    
    except Exception as e:
        self.logger.error(f"Failed to initialize FeatureAnalyzer: {e}")
        raise
    
    def analyze_feature(self, codebase_path: str, feature_description: str, output_format: str = 'json') -> Any:
    """
    Comprehensively analyze feature implementation requirements
    
    Args:
        codebase_path (str): Path to project codebase
        feature_description (str): Detailed feature requirements
        output_format (str, optional): Desired output format. Defaults to 'json'.
    
    Returns:
        Formatted analysis results based on specified output format
    """
    try:
        # Log the start of feature analysis
        self.logger.info(f"Starting feature analysis: {feature_description}")
        
        # Validate input paths
        if not os.path.exists(codebase_path):
            raise ValueError(f"Codebase path does not exist: {codebase_path}")
        
        # Perform comprehensive analysis using CodebaseAnalyzer
        analysis_result = self.codebase_analyzer.analyze_feature_requirement(
            codebase_path, 
            feature_description
        )
        
        # Log successful analysis
        self.logger.info(f"Feature analysis completed for: {feature_description}")
        
        # Format and return results
        formatted_result = self.result_formatter.format_result(
            analysis_result, 
            output_format
        )
        
        return formatted_result
    
    except Exception as e:
        # Log the error
        self.logger.error(f"Feature analysis failed: {e}")
        
        # Handle and format error based on output format
        return self._handle_error(e, output_format)

def _handle_error(self, error: Exception, output_format: str = 'json') -> Any:
    """
    Handle and format errors during feature analysis
    
    Args:
        error (Exception): The caught exception
        output_format (str, optional): Desired output format. Defaults to 'json'.
    
    Returns:
        Formatted error response
    """
    error_details = {
        'error': {
            'type': type(error).__name__,
            'message': str(error),
            'details': str(traceback.format_exc()) if self.logger.getEffectiveLevel() <= logging.DEBUG else None
        }
    }
    
    # Use result formatter to standardize error output
    return self.result_formatter.format_result(error_details, output_format)

# Example Usage
def main():
    analyzer = FeatureAnalyzer('config.yaml')
    result = analyzer.analyze_feature(
        '/path/to/project', 
        'Add user authentication with JWT support'
    )
    print(result)

if __name__ == '__main__':
    import argparse
    import sys
    import logging

    def main():
        # Create argument parser
        parser = argparse.ArgumentParser(
            description='Codebase Feature Analysis Tool',
            epilog='Analyze and suggest modifications for software features'
        )
        
        # Add arguments
        parser.add_argument(
            'codebase_path', 
            help='Path to the project codebase to analyze'
        )
        parser.add_argument(
            'feature_description', 
            help='Detailed description of the feature to implement'
        )
        parser.add_argument(
            '-c', '--config', 
            default=None, 
            help='Path to configuration file (optional)'
        )
        parser.add_argument(
            '-o', '--output', 
            choices=['json', 'yaml', 'text'], 
            default='json', 
            help='Output format for analysis results'
        )
        parser.add_argument(
            '-l', '--log-level', 
            choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
            default='INFO', 
            help='Set logging verbosity'
        )
        
        # Parse arguments
        try:
            args = parser.parse_args()
            
            # Configure logging based on user input
            logging.basicConfig(
                level=getattr(logging, args.log_level),
                format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            
            # Initialize FeatureAnalyzer
            try:
                analyzer = FeatureAnalyzer(
                    codebase_root=args.codebase_path, 
                    config_path=args.config, 
                    logging_level=args.log_level
                )
                
                # Perform feature analysis
                result = analyzer.analyze_feature(
                    args.codebase_path, 
                    args.feature_description, 
                    output_format=args.output
                )
                
                # Output results
                if args.output == 'json':
                    import json
                    print(json.dumps(result, indent=2))
                elif args.output == 'yaml':
                    import yaml
                    print(yaml.dump(result, default_flow_style=False))
                else:
                    print(result)
                
                sys.exit(0)
            
            except Exception as analyzer_error:
                logging.error(f"Feature analysis failed: {analyzer_error}")
                sys.exit(1)
        
        except argparse.ArgumentTypeError as arg_error:
            logging.error(f"Argument error: {arg_error}")
            parser.print_help()
            sys.exit(2)
    
    # Run the main function
    main()
